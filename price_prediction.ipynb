{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7392b282-25bd-468a-8037-b9526f2f39f1",
   "metadata": {},
   "source": [
    "# Price Prediction\n",
    "\n",
    "a project by Felix Marschall, Mika Scheid, Elias Müller and Jan Dorn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24a00ba-9e1d-4d50-bbd6-8a6d0f072485",
   "metadata": {},
   "source": [
    "## Notebook Structure\n",
    "\n",
    "1. **Introduction**: Overview of the project and objectives.\n",
    "2. **Dataset Overview**: Description of the dataset and its key components.\n",
    "3. **Data Preparation**: \n",
    "    - 3.1 Data consolidation\n",
    "    - 3.2 Data cleaning\n",
    "    - 3.3 Data transformation\n",
    "    - 3.4 Data reduction (feature engineering)\n",
    "4. **Modeling**: Implementation and comparison of different machine learning models.\n",
    "5. **Hyperparameter Tuning**: Optimization of model parameters.\n",
    "6. **Evaluation**: Analysis of model performance using various metrics.\n",
    "7. **Conclusion**: Summary of findings and future work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff827341",
   "metadata": {},
   "source": [
    "## 0. Installing and importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155f332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1634f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76afa41-7260-46c2-a2b8-779215a278d4",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This project aims to predict Airbnb listing prices using a dataset from Inside Airbnb. The focus is on applying data science techniques to preprocess, analyze, and model the data while exploring additional insights such as time-based patterns and geolocation analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42da9ae",
   "metadata": {},
   "source": [
    "## 2. Dataset Overview\n",
    "\n",
    "For each location (Amsterdam, Barcelona, Berlin, Budapest, Copenhagen, Istanbul, Lisbon, London, Oslo, Paris, Prague, Rome), various datasets are available from [Inside Airbnb](https://insideairbnb.com/get-the-data/). The most relevant ones are:\n",
    "\n",
    "- **listings.csv.gz**: Detailed listings data\n",
    "- **calendar.csv.gz**: Detailed calendar data\n",
    "- **reviews.csv.gz**: Detailed review data\n",
    "\n",
    "### **Hypothesis 1**: Reviews are not useful for price prediction\n",
    "\n",
    "We hypothesize that reviews do not contribute to price prediction, as they are influenced by the price itself. Higher-priced Airbnbs may receive lower ratings if expectations are not met, while cheaper ones may be rated higher due to better perceived value.\n",
    "\n",
    "Since reviews inherently reflect the price and do not provide independent insights into the accommodation's quality or value, we will exclude them from the initial price prediction model. Instead, we will focus on features like location, property type, size, and amenities, which have a direct impact on the price.\n",
    "\n",
    "All relevant data for this analysis is available in the **listings.csv.gz** file, which we downloaded for each city from [Inside Airbnb](https://insideairbnb.com/get-the-data/).\n",
    "\n",
    "### **Hypothesis 2**: Prices remain constant throughout the year\n",
    "\n",
    "One might assume that Airbnb prices change throughout the year depending on demand. However, this is not the case. \n",
    "\n",
    "Upon inspecting the **calendar.csv.gz** file for Amsterdam using SQL (due to the large size of the dataset, SQL is more efficient), we found that prices for every listing remain constant over the entire year.\n",
    "\n",
    "```sql\n",
    "SELECT listing_id, COUNT(DISTINCT price) AS num_distinct_prices\n",
    "FROM calendar\n",
    "GROUP BY listing_id\n",
    "ORDER BY num_distinct_prices DESC;\n",
    "```\n",
    "\n",
    "SQL Output:\n",
    "\n",
    "<img src=\"./images/hypothesis_2.png\" alt=\"SQL Output\" width=\"300\"/>\n",
    "\n",
    "As we can see from the output, no listing in the Amsterdam dataset has more than one price for the same listing throughout the given timeframe. We assume this holds true for every listing in every city, so we do not consider price fluctuations over time in this analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c177c9d7",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "Consists of Data Consolidation, Data Cleaning, Data Transformation and Data Reduction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383346f1",
   "metadata": {},
   "source": [
    "### 3.1 Data Consolidation\n",
    "\n",
    "The Airbnb listing data for each city is located in `./data/<city_name>/listings.csv.gz`. As a first step, we need to consolidate these datasets into a single dataframe for further analysis.\n",
    "\n",
    "Checking the columns of each dataset reveals the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a189c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data'\n",
    "\n",
    "for city in os.listdir(data_dir):\n",
    "    city_path = os.path.join(data_dir, city)\n",
    "    listings_path = os.path.join(city_path, 'listings.csv.gz')\n",
    "    if os.path.isdir(city_path) and os.path.exists(listings_path):\n",
    "        try:\n",
    "            df = pd.read_csv(listings_path, compression='gzip', nrows=1)\n",
    "            print(f\"{city}: {df.shape[1]} columns\")\n",
    "            # print(f\"Columns: {list(df.columns)}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {listings_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf2f539",
   "metadata": {},
   "source": [
    "The datasets have different numbers of columns. The datasets with 75 columns are missing the following columns:\n",
    "\n",
    "- availability_eoy\n",
    "- number_of_reviews_ly\n",
    "- estimated_occupancy_l365d\n",
    "- estimated_revenue_l365d\n",
    "\n",
    "In the consolidating script, the schema is defined to include all 79 columns. The datasets that are missing these 4 columns will have null values for these fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d341f06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "data_dir = './data'\n",
    "all_dfs = []\n",
    "success = True\n",
    "\n",
    "# define the desired schema\n",
    "schema = [\n",
    "    \"id\",\"listing_url\",\"scrape_id\",\"last_scraped\",\"source\",\"name\",\"description\",\"neighborhood_overview\",\"picture_url\",\n",
    "    \"host_id\",\"host_url\",\"host_name\",\"host_since\",\"host_location\",\"host_about\",\"host_response_time\",\"host_response_rate\",\n",
    "    \"host_acceptance_rate\",\"host_is_superhost\",\"host_thumbnail_url\",\"host_picture_url\",\"host_neighbourhood\",\n",
    "    \"host_listings_count\",\"host_total_listings_count\",\"host_verifications\",\"host_has_profile_pic\",\"host_identity_verified\",\n",
    "    \"neighbourhood\",\"neighbourhood_cleansed\",\"neighbourhood_group_cleansed\",\"latitude\",\"longitude\",\"property_type\",\n",
    "    \"room_type\",\"accommodates\",\"bathrooms\",\"bathrooms_text\",\"bedrooms\",\"beds\",\"amenities\",\"price\",\"minimum_nights\",\n",
    "    \"maximum_nights\",\"minimum_minimum_nights\",\"maximum_minimum_nights\",\"minimum_maximum_nights\",\"maximum_maximum_nights\",\n",
    "    \"minimum_nights_avg_ntm\",\"maximum_nights_avg_ntm\",\"calendar_updated\",\"has_availability\",\"availability_30\",\n",
    "    \"availability_60\",\"availability_90\",\"availability_365\",\"calendar_last_scraped\",\"number_of_reviews\",\n",
    "    \"number_of_reviews_ltm\",\"number_of_reviews_l30d\",\"availability_eoy\",\"number_of_reviews_ly\",\n",
    "    \"estimated_occupancy_l365d\",\"estimated_revenue_l365d\",\"first_review\",\"last_review\",\"review_scores_rating\",\n",
    "    \"review_scores_accuracy\",\"review_scores_cleanliness\",\"review_scores_checkin\",\"review_scores_communication\",\n",
    "    \"review_scores_location\",\"review_scores_value\",\"license\",\"instant_bookable\",\"calculated_host_listings_count\",\n",
    "    \"calculated_host_listings_count_entire_homes\",\"calculated_host_listings_count_private_rooms\",\n",
    "    \"calculated_host_listings_count_shared_rooms\",\"reviews_per_month\"\n",
    "]\n",
    "\n",
    "for city in os.listdir(data_dir):\n",
    "    city_path = os.path.join(data_dir, city)\n",
    "    listings_path = os.path.join(city_path, 'listings.csv.gz')\n",
    "    if os.path.isdir(city_path) and os.path.exists(listings_path):\n",
    "        try:\n",
    "            df = pd.read_csv(listings_path, compression='gzip', low_memory=False)\n",
    "            df = df.reindex(columns=schema)\n",
    "            all_dfs.append(df)\n",
    "            logging.info(f\"{city}: {df.shape[0]} listings loaded from {listings_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load {listings_path}: {e}\")\n",
    "            success = False\n",
    "    else:\n",
    "        logging.warning(f\"No listings.csv.gz found in {city_path}\")\n",
    "\n",
    "if all_dfs:\n",
    "    try:\n",
    "        combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        output_path = os.path.join(data_dir, 'all_listings_consolidated.csv')\n",
    "        combined_df.to_csv(output_path, index=False, na_rep='null')\n",
    "        logging.info(f\"Successfully combined all listings with defined schema. Final shape: {combined_df.shape}\")\n",
    "        logging.info(f\"Combined CSV saved to {output_path}\")\n",
    "\n",
    "        # validation of csv file\n",
    "        try:\n",
    "            test_df = pd.read_csv(output_path, low_memory=False)\n",
    "            logging.info(f\"Validation: Successfully read {output_path}. Shape: {test_df.shape}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Validation failed: Could not read {output_path}: {e}\")\n",
    "            success = False\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to concatenate or save combined DataFrame: {e}\")\n",
    "        success = False\n",
    "else:\n",
    "    logging.error(\"No dataframes to combine. Exiting.\")\n",
    "    success = False\n",
    "\n",
    "if success:\n",
    "    logging.info(\"Data consolidation completed successfully.\")\n",
    "else:\n",
    "    logging.error(\"Data consolidation encountered errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d53ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.read_csv(\"data/all_listings_consolidated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290fb8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16cbc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc425cfe",
   "metadata": {},
   "source": [
    "### 3.2 Data Cleaning\n",
    "\n",
    "#### 3.2.1 Handle missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e1a421",
   "metadata": {},
   "source": [
    "Columns with over 40% of missing values can be deleted. Too much missing data to be included in the price prediction model. This is part of 3.4 Data reduction but we already know we don't need these values so let's just do it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5871fb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of missing values per column and percentage\n",
    "missing_counts = combined_df.isnull().sum()\n",
    "missing_percent = (missing_counts / len(combined_df)) * 100\n",
    "\n",
    "# combine both in a dataframe for better overview\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_counts,\n",
    "    'Missing Percent': missing_percent\n",
    "})\n",
    "\n",
    "# sort by percentage for better overview in plot\n",
    "missing_df_sorted = missing_df.sort_values(by='Missing Percent', ascending=True)\n",
    "\n",
    "# color the bars red if they have more than 40% missing values\n",
    "bar_colors = ['red' if val > 40 else 'skyblue' for val in missing_df_sorted['Missing Percent']]\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.barh(missing_df_sorted.index, missing_df_sorted['Missing Percent'], color=bar_colors)\n",
    "plt.xlabel('Missing Value Percentage')\n",
    "plt.title('Percentage of Missing Values per Column')\n",
    "plt.axvline(x=40, color='red', linestyle='--', linewidth=1.5, label='40% threshold')\n",
    "plt.legend()\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# list of columns with more than 40% missing values\n",
    "columns_to_drop = missing_df[missing_df['Missing Percent'] > 40].index.tolist()\n",
    "\n",
    "# show\n",
    "print(f\"Columns with more than 40% missing values ({len(columns_to_drop)} total):\")\n",
    "for col in columns_to_drop:\n",
    "    print(f\"- {col}: {missing_df.loc[col, 'Missing Percent']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd08838a",
   "metadata": {},
   "source": [
    "Remove these columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b9e954",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_cleaned = combined_df.drop(columns=columns_to_drop)\n",
    "combined_df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191be082",
   "metadata": {},
   "source": [
    "Let's inspect the last remaining columns with missing values, their data type and an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dd7c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values\n",
    "missing_counts = combined_df_cleaned.isnull().sum()\n",
    "missing_percent = (missing_counts / len(combined_df)) * 100\n",
    "\n",
    "# filter only columns with missing values\n",
    "missing_columns = missing_counts[missing_counts > 0].index\n",
    "\n",
    "# list to store the results\n",
    "rows = []\n",
    "\n",
    "for col in missing_columns:\n",
    "    percent_missing = missing_percent[col]\n",
    "    dtype = combined_df[col].dtype\n",
    "    example_values = combined_df[col].dropna().unique()\n",
    "    example = example_values[0] if len(example_values) > 0 else \"—\"\n",
    "    \n",
    "    rows.append({\n",
    "        \"column\": col,\n",
    "        \"missing\": round(percent_missing, 2),\n",
    "        \"datatype\": dtype,\n",
    "        \"example\": example\n",
    "    })\n",
    "\n",
    "# create dataframe and sort\n",
    "missing_table = pd.DataFrame(rows)\n",
    "missing_table = missing_table.sort_values(by=\"missing\", ascending=False)\n",
    "\n",
    "# show table\n",
    "print(missing_table.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cfc141",
   "metadata": {},
   "source": [
    "We'll focus on a few key columns for targeted cleaning. For all other columns, missing values will be filled with the label \"ML\" (Missing Label).\n",
    "\n",
    "1. Since we're predicting the **price column**, we'll remove any rows where the price is missing (~27%). Although a trained model could estimate these prices, we can't verify their accuracy — and given the amount of available data, these entries don't add much value for now.\n",
    "2. **beds**: Median\n",
    "3. **bathrooms**: Median\n",
    "4. review_scores_value, review_scores_location, review_scores_checkin, review_scores_communication, review_scores_cleanliness, review_scores_accuracy,review_scores_rating: Median or Mean depending on distribution -> check distribution\n",
    "5. **bedrooms**: Median\n",
    "6. rest: \"ML\" or median depending on data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aab768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. remove rows with missing price\n",
    "combined_df_cleaned = combined_df_cleaned[combined_df_cleaned['price'].notna()]\n",
    "\n",
    "# 2. 3. 5.\n",
    "combined_df_cleaned['beds'] = combined_df_cleaned['beds'].fillna(combined_df_cleaned['beds'].median())\n",
    "combined_df_cleaned['bathrooms'] = combined_df_cleaned['bathrooms'].fillna(combined_df_cleaned['bathrooms'].median())\n",
    "combined_df_cleaned['bedrooms'] = combined_df_cleaned['bedrooms'].fillna(combined_df_cleaned['bedrooms'].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe4478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. check skewness\n",
    "print(\"Skewness review_scores_value:\", combined_df['review_scores_value'].skew())\n",
    "print(\"Skewness review_scores_location:\", combined_df['review_scores_location'].skew())\n",
    "print(\"Skewness review_scores_checkin:\", combined_df['review_scores_checkin'].skew())\n",
    "print(\"Skewness review_scores_communication:\", combined_df['review_scores_communication'].skew())\n",
    "print(\"Skewness review_scores_cleanliness:\", combined_df['review_scores_cleanliness'].skew())\n",
    "print(\"Skewness review_scores_accuracy:\", combined_df['review_scores_accuracy'].skew())\n",
    "print(\"Skewness review_scores_rating:\", combined_df['review_scores_rating'].skew())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b349225",
   "metadata": {},
   "source": [
    "The skewness of each each column is |col| > 0.5, so we choose the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fed543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. fill with median\n",
    "combined_df_cleaned['review_scores_value'] = combined_df_cleaned['review_scores_value'].fillna(combined_df_cleaned['review_scores_value'].median())\n",
    "combined_df_cleaned['review_scores_location'] = combined_df_cleaned['review_scores_location'].fillna(combined_df_cleaned['review_scores_location'].median())\n",
    "combined_df_cleaned['review_scores_checkin'] = combined_df_cleaned['review_scores_checkin'].fillna(combined_df_cleaned['review_scores_checkin'].median())\n",
    "combined_df_cleaned['review_scores_communication'] = combined_df_cleaned['review_scores_communication'].fillna(combined_df_cleaned['review_scores_communication'].median())\n",
    "combined_df_cleaned['review_scores_cleanliness'] = combined_df_cleaned['review_scores_cleanliness'].fillna(combined_df_cleaned['review_scores_cleanliness'].median())\n",
    "combined_df_cleaned['review_scores_accuracy'] = combined_df_cleaned['review_scores_accuracy'].fillna(combined_df_cleaned['review_scores_accuracy'].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d5d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. rest: \"ML\" or median\n",
    "combined_df_cleaned = combined_df_cleaned.copy()\n",
    "\n",
    "for col in combined_df_cleaned.columns:\n",
    "    if pd.api.types.is_numeric_dtype(combined_df_cleaned[col]):\n",
    "        median_value = combined_df_cleaned[col].median()\n",
    "        combined_df_cleaned[col] = combined_df_cleaned[col].fillna(median_value)\n",
    "    else:\n",
    "        combined_df_cleaned[col] = combined_df_cleaned[col].fillna(\"ML\")\n",
    "\n",
    "# Prüfung\n",
    "combined_df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b06cda",
   "metadata": {},
   "source": [
    "#### 3.2.2 Reduce noise\n",
    "\n",
    "First, identify the outliers. Only applicable on numerical columns. No interpretation of id columns possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2b021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract numerical columns\n",
    "numeric_cols = combined_df_cleaned.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# prepare number of subplots\n",
    "num_cols = len(numeric_cols)\n",
    "cols_per_row = 3\n",
    "num_rows = (num_cols + cols_per_row - 1) // cols_per_row\n",
    "\n",
    "# create boxplots\n",
    "fig, axes = plt.subplots(num_rows, cols_per_row, figsize=(cols_per_row * 5, num_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    axes[i].boxplot(combined_df_cleaned[col].dropna(), vert=False)\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].grid(True)\n",
    "\n",
    "# remove empty axes\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Boxplots der numerischen Spalten (Ausreißererkennung)\", fontsize=16, y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe8173",
   "metadata": {},
   "source": [
    "There are obvious outliers in the columns maximum_nights, minimum_maximum_nights, maximum_maximum_nights and maximum_nights_avg_ntm. We suspect these columns won't be needed in the price prediction models, so we proceed to the data transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aa9816",
   "metadata": {},
   "source": [
    "### 3.3 Data Transformation\n",
    "\n",
    "Now we transform the data into a format that is more suitable for modeling. This includes normalizing columns, adjusting data types if needed, discretizing values, and even creating new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee56ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb745a",
   "metadata": {},
   "source": [
    "We have identified the need for the following transformations (without normalization):\n",
    "1. **host_since**, **first_review**, **last_review** needs to be a datetime datatype\n",
    "2. **host_location** can be divided in city and country\n",
    "3. **price** needs to be a numerical value -> remove the dollar sign\n",
    "4. **host_response_rate** and **host_acceptance_rate** should be a numerical value\n",
    "5. **host_response_time** can be transformed into a numerical representation: within an hour -> 1; within a few hours -> 2; ...\n",
    "\n",
    "In some of these transformations, we need to set the previously set \"ML\" values to the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37ad345",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = combined_df_cleaned.copy()\n",
    "\n",
    "# 1. transform date columns to datetime and replace NaT with median\n",
    "date_cols = ['host_since', 'first_review', 'last_review']\n",
    "for col in date_cols:\n",
    "    # convert to datetime\n",
    "    df_transformed[col] = pd.to_datetime(df_transformed[col], errors='coerce')\n",
    "    # calculate median\n",
    "    median_date = df_transformed[col].median()\n",
    "    # replace NaT with median\n",
    "    df_transformed[col] = df_transformed[col].fillna(median_date)\n",
    "\n",
    "# 2. split host_location into host_city and host_country\n",
    "def split_location(val):\n",
    "    if isinstance(val, str) and ',' in val:\n",
    "        parts = val.split(',')\n",
    "        city = parts[0].strip()\n",
    "        country = parts[-1].strip()\n",
    "        return city, country\n",
    "    else:\n",
    "        return \"ML\", \"ML\"\n",
    "\n",
    "df_transformed[['host_city', 'host_country']] = df_transformed['host_location'].apply(\n",
    "    lambda x: pd.Series(split_location(x))\n",
    ")\n",
    "\n",
    "# 3. price: remove $-sign and commas, then convert to float\n",
    "df_transformed['price'] = (\n",
    "    df_transformed['price']\n",
    "    .astype(str)\n",
    "    .str.replace(r\"[\\$,]\", \"\", regex=True)\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "# 4. host_response_rate and host_acceptance_rate: remove % sign, convert to float and replace \"ML\" with median\n",
    "for col in ['host_response_rate', 'host_acceptance_rate']:\n",
    "    # replace % sign and convert to float\n",
    "    col_series = df_transformed[col].astype(str).str.replace('%', '', regex=True)\n",
    "    # calculate median\n",
    "    valid_values = col_series[col_series != 'ML'].astype(float)\n",
    "    median_val = valid_values.median()\n",
    "    # replace \"ML\" with median and convert to float\n",
    "    df_transformed[col] = col_series.replace('ML', str(median_val)).astype(float)\n",
    "\n",
    "# 5. host_response_time: transform into numerical representation\n",
    "df_transformed['host_response_time_num'] = df_transformed['host_response_time'].map({\n",
    "    'within an hour': 1,\n",
    "    'within a few hours': 2,\n",
    "    'within a day': 3,\n",
    "    'a few days or more': 4,\n",
    "    'ML': '5'\n",
    "}).astype(float)\n",
    "\n",
    "# check result\n",
    "print(\"host_since, first_review, last_review: \\n\", df_transformed[['host_since', 'first_review', 'last_review']].dtypes)\n",
    "print(\"host_city, host_country: \\n\", df_transformed[['host_city', 'host_country']].head())\n",
    "print(\"price: \\n\", df_transformed['price'].head())\n",
    "print(\"host_response_rate, host_acceptance_rate: \\n\", df_transformed[['host_response_rate', 'host_acceptance_rate']].head())\n",
    "print(\"host_response_time_num: \\n\", df_transformed['host_response_time_num'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfd17e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b16836",
   "metadata": {},
   "source": [
    "### 3.4 Data reduction\n",
    "\n",
    "Now we want to analyse which features to use for the price prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a162d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Correlation analysis for numerical features\n",
    "\n",
    "# Select only numerical columns and drop non-informative IDs\n",
    "num_cols = df_transformed.select_dtypes(include=['number']).columns.drop(['id', 'scrape_id', 'host_id'])\n",
    "corr = df_transformed[num_cols].corr()['price'].sort_values(ascending=False)\n",
    "\n",
    "print(\"Top correlated numerical features with price:\")\n",
    "print(corr.drop('price').head(10))\n",
    "print(\"\\nLeast correlated numerical features with price:\")\n",
    "print(corr.drop('price').tail(10))\n",
    "\n",
    "# Visualize correlation matrix for top features\n",
    "top_corr_features = corr.abs().sort_values(ascending=False).head(10).index\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df_transformed[top_corr_features].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix of Top Features\")\n",
    "plt.show()\n",
    "\n",
    "# Check unique values for categorical columns with potential predictive power\n",
    "cat_cols = [\n",
    "    'property_type', 'room_type', 'neighbourhood_cleansed', 'host_is_superhost',\n",
    "    'instant_bookable', 'host_country'\n",
    "]\n",
    "for col in cat_cols:\n",
    "    print(f\"\\n{col}: {df_transformed[col].nunique()} unique values\")\n",
    "    print(df_transformed[col].value_counts(dropna=False).head())\n",
    "\n",
    "# Recommendation: \n",
    "# - Use highly correlated numerical features (e.g., accommodates, bedrooms, beds, bathrooms, reviews_per_month, etc.)\n",
    "# - Use categorical features with moderate cardinality (room_type, property_type, neighbourhood_cleansed, host_is_superhost, instant_bookable, host_country)\n",
    "# - Drop columns with very high cardinality or little predictive value (e.g., name, description, url fields, host_url, picture_url, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
